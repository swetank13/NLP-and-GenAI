{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "f9d8f656-cada-4014-8b51-dfa331459c94",
   "metadata": {},
   "source": [
    "<center><h2>Gen AI: Key Concept and Foundation</h2></center"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "30179cb6-4389-4219-bd16-5d16b333dddd",
   "metadata": {},
   "source": [
    "#### 3.2: How LLMs work\n",
    "- Fundamentally LLMs use Autoregressive Generation.\n",
    "- System 1 thinking: Fast, Intuitive, automatic thinking (Ex: Recognize face, Drive on a known route).\n",
    "- System 2 thinking: Slow, Analytical and effortful (Ex: Getting married to a person, Building a business strategy).\n",
    "- LLMs are autoregressive models where they predict one token at a time and use tokens produced from the first iteration as input in the next one to produce new tokens.\n",
    "- Transformer architecture is the core of any LLM.\n",
    "- Training an LLM requires a huge amount of resources, and the cost associated with training is very high."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a98b3fbe-e311-4a8b-af20-0882ac55d7cd",
   "metadata": {},
   "source": [
    "#### 3.3: Context Window, Temperature, Top-p and Top-k\n",
    "- `Context Window` determines the number of past tokens an LLM considers when generating the next token, influencing coherence and memory.\n",
    "- `Temperature` controls randomness in text generation: Higher values → more creative outputs, Lower values → more deterministic results. It controls how random or creative the model's output will be.\n",
    "- `Top-p (nucleus) sampling` selects tokens from the smallest set whose cumulative probability exceeds p, dynamically adjusting the token pool size. Used for creative and diverse task (Ex: Chatbots, Storytelling).\n",
    "- `Top-k sampling` limits token selection to the top k most probable options, reducing randomness while maintaining diversity. Used for predictable, structured tasks (Ex: Writing code, Summarization).\n",
    "- Combining temperature, top-p, and top-k allows fine-tuning of model output, balancing creativity and relevance in text generation (Ex: k=10, p=0.9).\n",
    "- `Output length` set the output length as fixed."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4ab8498b-8977-4616-ad77-d6843c32f39a",
   "metadata": {},
   "source": [
    "#### 3.4: Challenges: Hallucinations, Security and Cost\n",
    "- why model Hallucinate ?\n",
    "  1. Predicting patterns vs True understanding.\n",
    "  2. Insufficient Training Data & Lack of Fine Tunning.\n",
    "  3. Incomplete and ambiguous prompts.\n",
    "- How to tackle Hallucinations ?\n",
    "  1. Representative dataset.\n",
    "  2. Fine Tunning and validation\n",
    "  3. Knowledge based systems\n",
    "- Security issue when user trick the system.\n",
    "- Cost: AI project cost is too much and ROI is doubtful."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "359b64e7-ada0-4e97-91b6-f3f5fa7246b1",
   "metadata": {},
   "source": [
    "#### 3.5: Vector Database\n",
    "- Embeddings are used for semantic (Based on meaning of the word) search.\n",
    "- Generate embedding in the form of vectors.\n",
    "- Query vector find the cosine similarity in vector database and return the result. value 1 means exact same.\n",
    "- What a vector database and how it works: https://www.pinecone.io/learn/vector-database/\n",
    "- Vector databases store and manage high-dimensional vector representations of data, enabling efficient similarity search and retrieval.\n",
    "- Databases such as Milvus, Pinecone, Qdrant, and Chromadb are gaining attention in the modern-day Gen AI boom.\n",
    "- They excel at handling unstructured data (text, images, audio) by converting it into embeddings using neural networks.\n",
    "- These databases use a combination of algorithms that support Approximate Nearest Neighbor (ANN) search. "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9dcb5660-8366-4898-aa4f-c6be7f982ba5",
   "metadata": {},
   "source": [
    "#### 3.6: RAG (Retrieval-Augmented Generation)\n",
    "- Retrieval-Augmented Generation (RAG) combines information retrieval with text generation, enhancing LLMs with external knowledge.\n",
    "- RAG retrieves relevant documents or data chunks and uses them to generate more accurate and context-aware responses.\n",
    "- This approach reduces hallucination by grounding outputs in factual information from the retrieved content.\n",
    "- RAG is widely used in question answering, chatbots, and document summarization to improve precision and relevance.\n",
    "- By dynamically incorporating up-to-date information, RAG allows LLMs to handle knowledge-intensive tasks beyond their training data."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.11"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
